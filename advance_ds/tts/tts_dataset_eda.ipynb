{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee59e44",
   "metadata": {},
   "source": [
    "# EDA on Speech Dataset That's Used for TTS Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccf9e0",
   "metadata": {},
   "source": [
    "### Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7771f9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LibriTTS/train-clean-100'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     31\u001b[39m                     metadata.append({\n\u001b[32m     32\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mspeaker_id\u001b[39m\u001b[33m\"\u001b[39m: speaker,\n\u001b[32m     33\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mchapter_id\u001b[39m\u001b[33m\"\u001b[39m: chapter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33maudio_path\u001b[39m\u001b[33m\"\u001b[39m: audio_path\n\u001b[32m     37\u001b[39m                     })\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(metadata)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m df = \u001b[43mload_libritts_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m df.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mload_libritts_metadata\u001b[39m\u001b[34m(data_dir)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_libritts_metadata\u001b[39m(data_dir=\u001b[33m'\u001b[39m\u001b[33mLibriTTS/train-clean-100\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      6\u001b[39m     metadata = []\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m speaker \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m      9\u001b[39m         speaker_dir = os.path.join(data_dir, speaker)\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(speaker_dir):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'LibriTTS/train-clean-100'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_libritts_metadata(data_dir='LibriTTS/train-clean-100'):\n",
    "    metadata = []\n",
    "    \n",
    "    for speaker in tqdm(os.listdir(data_dir)):\n",
    "        speaker_dir = os.path.join(data_dir, speaker)\n",
    "        if not os.path.isdir(speaker_dir):\n",
    "            continue\n",
    "\n",
    "        for chapter in os.listdir(speaker_dir):\n",
    "            chapter_dir = os.path.join(speaker_dir, chapter)\n",
    "            if not os.path.isdir(chapter_dir):\n",
    "                continue\n",
    "\n",
    "            transcript_file = os.path.join(chapter_dir, f'{speaker}_{chapter}.trans.txt')\n",
    "            if not os.path.exists(transcript_file):\n",
    "                continue\n",
    "\n",
    "            with open(transcript_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    file_id, text = line.split(' ', 1)\n",
    "                    audio_path = os.path.join(chapter_dir, f'{file_id}.wav')\n",
    "                    metadata.append({\n",
    "                        \"speaker_id\": speaker,\n",
    "                        \"chapter_id\": chapter,\n",
    "                        \"file_id\": file_id,\n",
    "                        \"text\": text,\n",
    "                        \"audio_path\": audio_path\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(metadata)\n",
    "\n",
    "df = load_libritts_metadata()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22cbca0",
   "metadata": {},
   "source": [
    "## Speaker Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8357107",
   "metadata": {},
   "source": [
    "**Number of unique speaker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ea807",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total speaker: \", df['speaker_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166f70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbc7c0c4",
   "metadata": {},
   "source": [
    "Gender Distribution (if metadata available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If gender info is in speaker IDs (e.g., 'M' or 'F' prefix)\n",
    "df['gender'] = df['speaker_id'].str[0].map({'M': 'Male', 'F': 'Female'})\n",
    "df['gender'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86baa39c",
   "metadata": {},
   "source": [
    "Recordings per Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7005c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_counts = df['speaker_id'].value_counts()\n",
    "speaker_counts.describe()  # mean, min, max\n",
    "speaker_counts.hist(bins=50)  # Check imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8218d36",
   "metadata": {},
   "source": [
    "**Insights**:\n",
    "- Is the dataset dominated by a few speakers?\n",
    "- Are genders balanced?\n",
    "- Are some speakers underrepresented?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d7536",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d8521",
   "metadata": {},
   "source": [
    "Sentence Length (Word & Character Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7094e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df[['word_count', 'char_count']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb52c96",
   "metadata": {},
   "source": [
    "Vocabulary & Phoneme Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68102a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Unique words\n",
    "all_words = ' '.join(df['text']).lower().split()\n",
    "word_counts = Counter(all_words)\n",
    "print(\"Top 20 words:\", word_counts.most_common(20))\n",
    "\n",
    "# Phoneme analysis (requires G2P model)\n",
    "!pip install g2p-en\n",
    "from g2p_en import G2p\n",
    "g2p = G2p()\n",
    "\n",
    "def text_to_phonemes(text):\n",
    "    return ' '.join(g2p(text))\n",
    "\n",
    "df['phonemes'] = df['text'].apply(text_to_phonemes)\n",
    "all_phonemes = ' '.join(df['phonemes']).split()\n",
    "phoneme_counts = Counter(all_phonemes)\n",
    "print(\"Top 20 phonemes:\", phoneme_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c38bb2",
   "metadata": {},
   "source": [
    "**Insights**:\n",
    "- Are there very short/long sentences?\n",
    "- Are rare words or phonemes missing?\n",
    "- Does the dataset cover diverse linguistic patterns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029847b",
   "metadata": {},
   "source": [
    "## Audio Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfa0ef",
   "metadata": {},
   "source": [
    "Audio Duration Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b384ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "def get_duration(audio_path):\n",
    "    try:\n",
    "        return librosa.get_duration(filename=audio_path)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['duration'] = df['audio_path'].apply(get_duration)\n",
    "df['duration'].describe()  # Check min, max, mean\n",
    "df['duration'].hist(bins=100)  # Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc057aaf",
   "metadata": {},
   "source": [
    "Sample Rate & Channel Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_info(audio_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        return sr, y.shape[0]\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "df['sample_rate'] = df['audio_path'].apply(lambda x: get_audio_info(x)[0])\n",
    "df['sample_rate'].value_counts()  # Should be consistent (e.g., 22050 Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68b663",
   "metadata": {},
   "source": [
    "**Insights**:\n",
    "- Are there extremely short/long clips?\n",
    "- Is the sample rate consistent?\n",
    "- Are there silent or corrupted files?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280cbdfc",
   "metadata": {},
   "source": [
    "## Speaker-Text-Audio Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b6f71",
   "metadata": {},
   "source": [
    "Do some speakers have longer/shorter sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('speaker_id')['word_count'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cbf76",
   "metadata": {},
   "source": [
    "Do certain phonemes appear more with certain speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check if nasal sounds (/m/, /n/) vary by speaker\n",
    "df['nasal_count'] = df['phonemes'].apply(lambda x: x.count('m') + x.count('n'))\n",
    "df.groupby('speaker_id')['nasal_count'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae8bfe",
   "metadata": {},
   "source": [
    "**Insights**:\n",
    "- Are some speakers overrepresented in certain linguistic patterns?\n",
    "- Are there dialectal variations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd12e3",
   "metadata": {},
   "source": [
    "## Automated EDA Tools\n",
    "`pandas-profiling`: Quick overview of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile.to_widgets()\n",
    "# profile.to_file(\"libritts_eda.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
