{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e383d9b9",
   "metadata": {},
   "source": [
    "# Spark SQL with PySpark\n",
    "**Objective**\n",
    "\n",
    "Learn how to use Spark SQL in PySpark to perform data manipulation and analysis on structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df72f6",
   "metadata": {},
   "source": [
    "## Setup Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2c5d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/rnd/hieng/venv/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/rnd/hieng/venv/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d607e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 02:59:42 WARN Utils: Your hostname, vishnu resolves to a loopback address: 127.0.1.1; using 103.16.62.251 instead (on interface eno8303)\n",
      "25/04/28 02:59:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/28 02:59:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLLab\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076bbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data into a DataFrame\n",
    "# DOES NOT WORK IN COLAB\n",
    "# employees_df = spark.read.json(\"./employees.json\")\n",
    "# departments_df = spark.read.json(\"./departments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a288e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file into Python Dict\n",
    "import json\n",
    "with open(\"./employees.json\") as f:\n",
    "    employees = json.load(f)\n",
    "with open(\"./departments.json\") as f:\n",
    "    departments = json.load(f)\n",
    "# Create DataFrames from the loaded data\n",
    "employees_df = spark.createDataFrame(employees)\n",
    "departments_df = spark.createDataFrame(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54513b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views for SQL queries\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd9f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+------+\n",
      "|dept_id| id|   name|salary|\n",
      "+-------+---+-------+------+\n",
      "|    101|  1|  Alice| 60000|\n",
      "|    102|  2|    Bob| 70000|\n",
      "|    103|  3|Charlie| 80000|\n",
      "|    104|  4|  David| 90000|\n",
      "|    105|  5|    Eve|100000|\n",
      "|    106|  6|  Frank|110000|\n",
      "|    107|  7|  Grace|120000|\n",
      "|    108|  8|  Heidi|130000|\n",
      "|    109|  9|   Ivan|140000|\n",
      "|    110| 10|   Judy|150000|\n",
      "|    101| 11|   Karl|160000|\n",
      "|    102| 12|    Leo|170000|\n",
      "|    103| 13|Mallory|180000|\n",
      "|    104| 14|   Nina|190000|\n",
      "|    105| 15|  Oscar|200000|\n",
      "|    106| 16|  Peggy|210000|\n",
      "|    107| 17|Quentin|220000|\n",
      "|    108| 18| Rupert|230000|\n",
      "|    109| 19|  Sybil|240000|\n",
      "|    110| 20|  Trent|250000|\n",
      "+-------+---+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1c16c",
   "metadata": {},
   "source": [
    "## Lab Tasks\n",
    "### Task 1: Basic Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f8e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+------+\n",
      "|dept_id| id|   name|salary|\n",
      "+-------+---+-------+------+\n",
      "|    101|  1|  Alice| 60000|\n",
      "|    102|  2|    Bob| 70000|\n",
      "|    103|  3|Charlie| 80000|\n",
      "|    104|  4|  David| 90000|\n",
      "|    105|  5|    Eve|100000|\n",
      "|    106|  6|  Frank|110000|\n",
      "|    107|  7|  Grace|120000|\n",
      "|    108|  8|  Heidi|130000|\n",
      "|    109|  9|   Ivan|140000|\n",
      "|    110| 10|   Judy|150000|\n",
      "|    101| 11|   Karl|160000|\n",
      "|    102| 12|    Leo|170000|\n",
      "|    103| 13|Mallory|180000|\n",
      "|    104| 14|   Nina|190000|\n",
      "|    105| 15|  Oscar|200000|\n",
      "|    106| 16|  Peggy|210000|\n",
      "|    107| 17|Quentin|220000|\n",
      "|    108| 18| Rupert|230000|\n",
      "|    109| 19|  Sybil|240000|\n",
      "|    110| 20|  Trent|250000|\n",
      "+-------+---+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query to select all employ\n",
    "empl = spark.sql(\"SELECT * FROM employees\")\n",
    "empl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0b0e8",
   "metadata": {},
   "source": [
    "### Task 2: Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73083db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "+-------+---+-------+------+\n",
      "|dept_id| id|   name|salary|\n",
      "+-------+---+-------+------+\n",
      "|    106| 26|   Zane|310000|\n",
      "|    107| 27|  Aaron|320000|\n",
      "|    108| 28|  Bella|330000|\n",
      "|    109| 29|  Cathy|340000|\n",
      "|    110| 30|  Derek|350000|\n",
      "|    101| 31|  Ethan|360000|\n",
      "|    102| 32|  Fiona|370000|\n",
      "|    103| 33| George|380000|\n",
      "|    104| 34| Hannah|390000|\n",
      "|    105| 35|    Ian|400000|\n",
      "|    106| 36|Jasmine|410000|\n",
      "|    107| 37|  Kevin|420000|\n",
      "|    108| 38|  Laura|430000|\n",
      "|    109| 39|   Mike|440000|\n",
      "|    110| 40|   Nina|450000|\n",
      "|    101| 41|  Oscar|460000|\n",
      "|    102| 42|   Paul|470000|\n",
      "|    103| 43|  Quinn|480000|\n",
      "|    104| 44| Rachel|490000|\n",
      "|    105| 45|  Steve|500000|\n",
      "+-------+---+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a query to find employees with a salary greater than 100,000.\n",
    "high_salary_employees = spark.sql(\"SELECT * FROM employees WHERE salary > 300000\")\n",
    "print(high_salary_employees.count())  # Count the number of high salary employees\n",
    "high_salary_employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de12f6",
   "metadata": {},
   "source": [
    "### Task 3: Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ef9699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:======================================================> (47 + 1) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|dept_id|avg_salary|\n",
      "+-------+----------+\n",
      "|    101|  310000.0|\n",
      "|    102|  320000.0|\n",
      "|    103|  280000.0|\n",
      "|    104|  290000.0|\n",
      "|    105|  300000.0|\n",
      "|    106|  310000.0|\n",
      "|    107|  320000.0|\n",
      "|    108|  330000.0|\n",
      "|    109|  340000.0|\n",
      "|    110|  350000.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write a query to calculate the average salary per department (dept_id).\n",
    "avg_salary_per_dept = spark.sql(\"\"\"\n",
    "    SELECT dept_id, AVG(salary) AS avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY dept_id\n",
    "\"\"\")\n",
    "avg_salary_per_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44994d5",
   "metadata": {},
   "source": [
    "### Task 4: Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc0fe25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============>  (41 + 7) / 48][Stage 34:>               (1 + 46) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+------+\n",
      "|employee_name|department_name|salary|\n",
      "+-------------+---------------+------+\n",
      "|        Alice|             HR| 60000|\n",
      "|         Karl|             HR|160000|\n",
      "|          Uma|             HR|260000|\n",
      "|        Ethan|             HR|360000|\n",
      "|        Oscar|             HR|460000|\n",
      "|       Yvonne|             HR|560000|\n",
      "|          Bob|    Engineering| 70000|\n",
      "|          Leo|    Engineering|170000|\n",
      "|       Victor|    Engineering|270000|\n",
      "|        Fiona|    Engineering|370000|\n",
      "|         Paul|    Engineering|470000|\n",
      "|         Zach|    Engineering|570000|\n",
      "|      Charlie|      Marketing| 80000|\n",
      "|      Mallory|      Marketing|180000|\n",
      "|       Walter|      Marketing|280000|\n",
      "|       George|      Marketing|380000|\n",
      "|        Quinn|      Marketing|480000|\n",
      "|        David|        Finance| 90000|\n",
      "|         Nina|        Finance|190000|\n",
      "|         Xena|        Finance|290000|\n",
      "+-------------+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write a query to join the employees and departments tables and display the employee name, department name, and salary.\n",
    "join_query = \"\"\"\n",
    "SELECT e.name AS employee_name, d.dept_name AS department_name, e.salary \n",
    "FROM employees e \n",
    "JOIN departments d \n",
    "ON e.dept_id = d.dept_id\n",
    "\"\"\"\n",
    "joined_df = spark.sql(join_query)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3e254",
   "metadata": {},
   "source": [
    "### Task 5: Complex Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "601ca15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:======================================================> (47 + 1) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|dept_name|avg_salary|\n",
      "+---------+----------+\n",
      "|    Admin|  350000.0|\n",
      "+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write a query to find the department(s) with the highest average salary. \n",
    "# Use a subquery or common table expression (CTE).\n",
    "highest_avg_salary_query = \"\"\"\n",
    "WITH avg_salaries AS (\n",
    "    SELECT dept_id, AVG(salary) AS avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY dept_id\n",
    ")\n",
    "SELECT d.dept_name, a.avg_salary\n",
    "FROM avg_salaries a\n",
    "JOIN departments d\n",
    "ON a.dept_id = d.dept_id\n",
    "WHERE a.avg_salary = (SELECT MAX(avg_salary) FROM avg_salaries)\n",
    "\"\"\"\n",
    "highest_avg_salary_df = spark.sql(highest_avg_salary_query)\n",
    "highest_avg_salary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262e5b5",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3988a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d565f",
   "metadata": {},
   "source": [
    "## (My) Key Takeaway\n",
    "From this Lab practice, Spark SQL, I have learned that Spark SQL provides a powerfull interface for querying structured data using SQL syntax within PySpark. It allows us to create temporary views from DataFrames and perform operations like selection, filtering, aggregation, and joins efficiently. We also explored how to use Spark SQL for complex queries, such as finding departments with the highest average salary using subqueries or CTEs. Overall, Spark SQL simplifies data manipulation and analysis on large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
