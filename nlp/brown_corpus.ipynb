{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Text Generation on Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/maohieng/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 1161192\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK Brown Corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Combine all words into a single string\n",
    "# corpus = ' '.join(brown.words())\n",
    "# print('Corpus length:', len(corpus))\n",
    "words = brown.words()\n",
    "print('Number of words:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 812834\n",
      "Validation data length: 116119\n",
      "Test data length: 232239\n",
      "['dissolve', 'the', 'an', 'spite', 'of', 'other', 'work', 'full', '.', 'accept']\n"
     ]
    }
   ],
   "source": [
    "# Split the corpus into training (70%), validation (10%), and test (20%) sets\n",
    "train_data, tmp_data = train_test_split(words, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(tmp_data, test_size=2/3, random_state=42)\n",
    "\n",
    "print('Train data length:', len(train_data))\n",
    "print('Validation data length:', len(val_data))\n",
    "print('Test data length:', len(test_data))\n",
    "print(train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 56057\n"
     ]
    }
   ],
   "source": [
    "total_vocab = set(words)\n",
    "print('Total vocabulary size:', len(total_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.write(' '.join(train_data))\n",
    "\n",
    "with open('val.txt', 'w') as f:\n",
    "    f.write(' '.join(val_data))\n",
    "\n",
    "with open('test.txt', 'w') as f:\n",
    "    f.write(' '.join(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "with open('train.txt', 'r') as f:\n",
    "    train_data = f.read().split()\n",
    "\n",
    "with open('val.txt', 'r') as f:\n",
    "    val_data = f.read().split()\n",
    "\n",
    "with open('test.txt', 'r') as f:\n",
    "    test_data = f.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data vocabulary size: 56057\n"
     ]
    }
   ],
   "source": [
    "vocab_counter = Counter(words)\n",
    "print('Data vocabulary size:', len(vocab_counter))\n",
    "print('Most common words:', vocab_counter.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens: 50000\n",
      "[('the', 62713), (',', 58334), ('.', 49346), ('of', 36080), ('and', 27915), ('to', 25732), ('a', 21881), ('in', 19536), ('that', 10237), ('is', 10011)]\n",
      "Remaining tokens: 6057\n",
      "['hey', 'dammit', 'cuff', 'nightshirt', \"Pa'd\", 'shit-sick', \"Pa's\", 'pa', \"What'd\", 'sassing']\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocabulary size \n",
    "vocab_size = 50000\n",
    "\n",
    "# Create a dictionary with 'vocab_size' most common words\n",
    "most_common_tokens = vocab_counter.most_common(vocab_size)\n",
    "print('Most common tokens:', len(most_common_tokens))\n",
    "print(most_common_tokens[:10])\n",
    "\n",
    "list_remaining_tokens = list(vocab_counter.keys())[vocab_size:]\n",
    "print('Remaining tokens:', len(list_remaining_tokens))\n",
    "print(list_remaining_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_tokens(data, list_remaining_tokens):\n",
    "    result = []\n",
    "    for token in data:\n",
    "        if token in list_remaining_tokens:\n",
    "            result.append('<unk>')\n",
    "        else:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite replace_rare_tokens function using simple loop\n",
    "def replace_rare_tokens(data, most_common_tokens):\n",
    "    result = []\n",
    "    for word in data:\n",
    "        if word in most_common_tokens:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append('<UNK>')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training tokens: ['dissolve', 'the', 'an', 'spite', 'of', 'other', 'work', 'full', '.', 'accept']\n",
      "Sample validation tokens: ['a', 'defense', 'appropriate', 'by', 'can', 'not', 'should', 'manner', 'He', 'films']\n",
      "Sample test tokens: ['delegates', 'used', 'I', 'witty', 'Ancel', '<unk>', 'immigrant', '.', '?', 'now']\n"
     ]
    }
   ],
   "source": [
    "train_tokens_limited = replace_unk_tokens(train_data, list_remaining_tokens)\n",
    "val_tokens_limited = replace_unk_tokens(val_data, list_remaining_tokens)\n",
    "test_tokens_limited = replace_unk_tokens(test_data, list_remaining_tokens)\n",
    "\n",
    "print(\"Sample training tokens:\", train_tokens_limited[:10])\n",
    "print(\"Sample validation tokens:\", val_tokens_limited[:10])\n",
    "print(\"Sample test tokens:\", test_tokens_limited[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open('train_limited.txt', 'w') as f:\n",
    "    f.write(' '.join(train_tokens_limited))\n",
    "\n",
    "with open('val_limited.txt', 'w') as f:\n",
    "    f.write(' '.join(val_tokens_limited))\n",
    "\n",
    "with open('test_limited.txt', 'w') as f:\n",
    "    f.write(' '.join(test_tokens_limited))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load limited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "with open('train_limited.txt', 'r') as f:\n",
    "    train_tokens_limited = f.read().split()\n",
    "\n",
    "with open('val_limited.txt', 'r') as f:\n",
    "    val_tokens_limited = f.read().split()\n",
    "\n",
    "with open('test_limited.txt', 'r') as f:\n",
    "    test_tokens_limited = f.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate n-grams\n",
    "def generate_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1-gram: [(1,), (2,), (3,), (4,), (5,)]\n",
      "Test 2-gram: [(1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "Test 3-gram: [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
      "Test 4-gram: [(1, 2, 3, 4), (2, 3, 4, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Test generate_ngrams function\n",
    "test_tokens = [1, 2, 3, 4, 5]\n",
    "for i in range (1, 5):\n",
    "    ngrams = generate_ngrams(test_tokens, i)\n",
    "    print(f\"Test {i}-gram:\", ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.Counter'>, {1: Counter({(1,): 1, (2,): 1, (3,): 1, (4,): 1, (5,): 1}), 2: Counter({(1, 2): 1, (2, 3): 1, (3, 4): 1, (4, 5): 1}), 3: Counter({(1, 2, 3): 1, (2, 3, 4): 1, (3, 4, 5): 1}), 4: Counter({(1, 2, 3, 4): 1, (2, 3, 4, 5): 1})})\n",
      "Counter({(): 5, (1,): 1, (2,): 1, (3,): 1, (4,): 1, (1, 2): 1, (2, 3): 1, (3, 4): 1, (1, 2, 3): 1, (2, 3, 4): 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "test_ngram_counts = defaultdict(Counter)\n",
    "test_context_counts = Counter()\n",
    "for i in range(1, 4+1):\n",
    "    ngrams = generate_ngrams(test_tokens, i)\n",
    "    for ngram in ngrams:\n",
    "        test_ngram_counts[len(ngram)][ngram] += 1\n",
    "        test_context_counts[ngram[:-1]] += 1\n",
    "\n",
    "print(test_ngram_counts)\n",
    "print(test_context_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Model 1: Backoff Method without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def l1_train(tokens, n):\n",
    "    ngram_counts = defaultdict(Counter)\n",
    "    context_counts = Counter()\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        ngrams = generate_ngrams(tokens, i)\n",
    "        for ngram in ngrams:\n",
    "            ngram_counts[len(ngram)][ngram] += 1\n",
    "            context_counts[ngram[:-1]] += 1\n",
    "\n",
    "    return ngram_counts, context_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities without smoothing\n",
    "def l1_probability(ngram_counts, context_counts, ngram):\n",
    "    for i in range(len(ngram), 0, -1):\n",
    "        if ngram[-i:] in ngram_counts[i]:\n",
    "            return ngram_counts[i][ngram[-i:]] / context_counts[ngram[-i:-1]]\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "train_ngram_counts, train_context_counts = l1_train(train_tokens_limited, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train n-gram counts: 233353\n",
      "Train context counts: 233353\n"
     ]
    }
   ],
   "source": [
    "print(\"Train n-gram counts:\", train_ngram_counts[1][('i',)])\n",
    "print(\"Train context counts:\", train_context_counts[('i',)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of (' ', ' '): 0.1892496649135914\n"
     ]
    }
   ],
   "source": [
    "test_l1 = (' ', ' ')\n",
    "print(f\"Probability of {test_l1}: {l1_probability(train_ngram_counts, train_context_counts, test_l1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5621\n"
     ]
    }
   ],
   "source": [
    "test2_l1 = train_ngram_counts[4][(' ', ' ', ' ', ' ')]\n",
    "print(test2_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "        self.vocabs = None\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def train(self, tokens):\n",
    "        self.vocabs = set(tokens)\n",
    "        self.vocab_size = len(self.vocabs)\n",
    "        for i in range(1, self.n+1):\n",
    "            ngrams = generate_ngrams(tokens, i)\n",
    "            for ngram in ngrams:\n",
    "                self.ngram_counts[len(ngram)][ngram] += 1\n",
    "                self.context_counts[ngram[:-1]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Backoff No Smoothing Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_no_smooth_prob(model, target):\n",
    "    for i in range(len(target), 0, -1):\n",
    "        if target[-i:] in model.ngram_counts[i]:\n",
    "            count = model.ngram_counts[i][target[-i:]]\n",
    "            context_count = model.context_counts[target[-i:-1]]\n",
    "            return count / context_count\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_prob(model, target, lambdas, k=1):\n",
    "    prob = 0.0\n",
    "    for i in range(1, model.n + 1):\n",
    "        count = model.ngram_counts[i][target[-i:]]\n",
    "        context_count = model.context_counts[target[-i:-1]]\n",
    "        prob += lambdas[i-1] * ((count + k) / (context_count + k * model.vocab_size))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LanguageModel(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.train(train_tokens_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43025\n",
      "Train 1-gram counts: 43025\n",
      "Train 2-gram counts: 464560\n",
      "Train 3-gram counts: 751941\n",
      "Train 4-gram counts: 809318\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", model1.vocab_size)\n",
    "print(\"Train 1-gram counts:\", len(model1.ngram_counts[1]))\n",
    "print(\"Train 2-gram counts:\", len(model1.ngram_counts[2]))\n",
    "print(\"Train 3-gram counts:\", len(model1.ngram_counts[3]))\n",
    "print(\"Train 4-gram counts:\", len(model1.ngram_counts[4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ngrams = ('the', 'fox', 'jumps', 'over')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backoff No Smoothing Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010346516016800477\n"
     ]
    }
   ],
   "source": [
    "test_prob = backoff_no_smooth_prob(model1, test_ngrams)\n",
    "print(test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00042762520991898026\n"
     ]
    }
   ],
   "source": [
    "# these values achieved from the testing below\n",
    "best_lambdas = [0.4, 0.3, 0.2, 0.1] \n",
    "best_k = 0.0025\n",
    "\n",
    "p2 = interpolation_prob(model1, test_ngrams, best_lambdas, best_k)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ('the', 'fox', 'jumps', 'over')\n",
      "1-gram\n",
      "- target: ('over',)\n",
      "- count: 841\n",
      "- context: ()\n",
      "- context count: 812834\n",
      "- lambda: 0.4\n",
      "- probability: 0.00041385295956932806\n",
      "2-gram\n",
      "- target: ('jumps', 'over')\n",
      "- count: 0\n",
      "- context: ('jumps',)\n",
      "- context count: 0\n",
      "- lambda: 0.3\n",
      "- probability: 4.28510212826739e-05\n",
      "3-gram\n",
      "- target: ('fox', 'jumps', 'over')\n",
      "- count: 0\n",
      "- context: ('fox', 'jumps')\n",
      "- context count: 0\n",
      "- lambda: 0.2\n",
      "- probability: 2.85673475217826e-05\n",
      "4-gram\n",
      "- target: ('the', 'fox', 'jumps', 'over')\n",
      "- count: 0\n",
      "- context: ('the', 'fox', 'jumps')\n",
      "- context count: 0\n",
      "- lambda: 0.1\n",
      "- probability: 1.42836737608913e-05\n",
      "Final interopolation probability: 0.0004995550021346758\n"
     ]
    }
   ],
   "source": [
    "prob = 0.0\n",
    "print(\"Sentence:\", test_ngrams)\n",
    "for i in range(1, 5):\n",
    "    print(f'{i}-gram')\n",
    "    print(\"- target:\", test_ngrams[-i:])\n",
    "    count = model1.ngram_counts[i][test_ngrams[-i:]]\n",
    "    print('- count:', count)\n",
    "    print('- context:', test_ngrams[-i:-1])\n",
    "    context_count = model1.context_counts[test_ngrams[-i:-1]]\n",
    "    print('- context count:', context_count)\n",
    "    print('- lambda:', best_lambdas[i-1])\n",
    "    p = best_lambdas[i-1] * ((count + best_k) / (context_count + best_k * model1.vocab_size))\n",
    "    print('- probability:', p)\n",
    "    prob += p\n",
    "\n",
    "print(\"Final interopolation probability:\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backoff Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perplexity_backoff_no_smooth(model, tokens, n):\n",
    "    ngrams = generate_ngrams(tokens, n)\n",
    "    log_prob_sum = 0\n",
    "    for ngram in ngrams:\n",
    "        prob = backoff_no_smooth_prob(model, ngram)\n",
    "        log_prob_sum += np.log(prob + 1e-12)  # Avoid log(0) by adding a small constant\n",
    "    perplexity = np.exp(-log_prob_sum / len(ngrams))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff no smoothing perplexity: 1519.8483250858735\n"
     ]
    }
   ],
   "source": [
    "l1_perplexity = perplexity_backoff_no_smooth(model1, val_tokens_limited, 4)\n",
    "print(\"Backoff no smoothing perplexity:\", l1_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perplexity_interpolation(model, tokens, n, lambdas, k):\n",
    "    ngrams = generate_ngrams(tokens, n)\n",
    "    log_prob_sum = 0\n",
    "    for ngram in ngrams:\n",
    "        prob = interpolation_prob(model, ngram, lambdas, k)\n",
    "        log_prob_sum += np.log(prob + 1e-12)  # Avoid log(0) by adding a small constant\n",
    "    perplexity = np.exp(-log_prob_sum / len(ngrams))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation perplexity: 1658.456978220652\n"
     ]
    }
   ],
   "source": [
    "inter_pp = perplexity_interpolation(model1, val_tokens_limited, 4, best_lambdas, best_k)\n",
    "print(\"Interpolation perplexity:\", inter_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(val_tokens, lambda_list, k_values):\n",
    "    best_perplexity = float('inf')\n",
    "    best_lambda, best_k = None, None\n",
    "    \n",
    "    for lambdas in lambda_list:\n",
    "        for k in k_values:\n",
    "            perplexity = perplexity_interpolation(model1, val_tokens, 4, lambdas, k)\n",
    "            if perplexity < best_perplexity:\n",
    "                best_perplexity = perplexity\n",
    "                best_lambda = lambdas\n",
    "                best_k = k\n",
    "    return best_lambda, best_k, best_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: [0.4, 0.3, 0.2, 0.1]\n",
      "Best k: 0.05\n",
      "Best perplexity: 483.3501313290752\n"
     ]
    }
   ],
   "source": [
    "lambdas_list = [[0.1, 0.2, 0.3, 0.4], [0.25, 0.25, 0.25, 0.25], [0.4, 0.3, 0.2, 0.1]]\n",
    "k_values = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "best_lambda, best_k, best_perplexity = tune_hyperparameters(val_tokens_limited, lambdas_list, k_values)\n",
    "print(\"Best lambda:\", best_lambda)\n",
    "print(\"Best k:\", best_k)\n",
    "print(\"Best perplexity:\", best_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_k_parameters(val_tokens, current_k, step=0.05, max_step=100):\n",
    "    if max_step == 0:\n",
    "        return current_k, 0\n",
    "\n",
    "    perplexity = perplexity_interpolation(model1, val_tokens, 4, best_lambda, current_k)\n",
    "    next_perplexity = perplexity_interpolation(model1, val_tokens, 4, best_lambda, current_k*step)\n",
    "    if  next_perplexity < perplexity:\n",
    "        current_k *= step\n",
    "        return tune_k_parameters(val_tokens, current_k, step, max_step-1)\n",
    "    else:\n",
    "        return current_k, max_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best k: 0.0025000000000000005\n",
      "Steps: 100\n"
     ]
    }
   ],
   "source": [
    "new_best_k, steps = tune_k_parameters(val_tokens_limited, best_k)\n",
    "print(\"New best k:\", new_best_k)\n",
    "print(\"Steps:\", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation perplexity: 1658.456978220652\n"
     ]
    }
   ],
   "source": [
    "best_lambdas = [0.4, 0.3, 0.2, 0.1]\n",
    "best_k = 0.0025\n",
    "inter_pp = perplexity_interpolation(model1, val_tokens_limited, 4, best_lambdas, best_k)\n",
    "print(\"Interpolation perplexity:\", inter_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start context: ['the', 'fox', 'jumps', 'over']\n",
      "Generated: ['the', 'fox', 'jumps', 'over']\n"
     ]
    }
   ],
   "source": [
    "start_context = test_ngrams\n",
    "contexts = list(start_context)\n",
    "generated = contexts[:]\n",
    "\n",
    "print(\"Start context:\", contexts)\n",
    "print(\"Generated:\", generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next candidates: the\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for word in model1.ngram_counts[1]:\n",
    "    target_context = tuple(contexts + [word[0]])\n",
    "    # print(\"Target:\", target_context)\n",
    "    prob = interpolation_prob(model1, target_context, best_lambdas, best_k)\n",
    "    candidates.append((word, prob))\n",
    "\n",
    "candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "selected_next_word = candidates[0][0][0]\n",
    "if selected_next_word == '<UNK>':\n",
    "    selected_next_word = candidates[1][0][0]\n",
    "\n",
    "print(\"Next candidates:\", selected_next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_interpolation(model, start_context, lambdas, k, max_length=100):\n",
    "    contexts = list(start_context)\n",
    "    generated = contexts[:]\n",
    "    \n",
    "    for _ in range(max_length -  len(contexts)):\n",
    "        candidates = []\n",
    "        for word in model.ngram_counts[1]:\n",
    "            target_context = tuple(contexts + [word[0]])\n",
    "            prob = interpolation_prob(model, target_context, lambdas, k)\n",
    "            candidates.append((word, prob))\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_word = candidates[0][0]\n",
    "        if selected_word[0] == '<UNK>':\n",
    "            selected_word = candidates[1][0]\n",
    "\n",
    "        if selected_word[0] == '<END>':\n",
    "            break\n",
    "\n",
    "        if selected_word[0] == contexts[-1]:\n",
    "            selected_word = candidates[2][0]\n",
    "        \n",
    "        generated.append(selected_word[0])\n",
    "        contexts.append(selected_word[0])\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the  quick  brown  the  .  ,  the  .  ,  the  .  ,  the  .  ,  the  .  ,  the  .\n"
     ]
    }
   ],
   "source": [
    "gen_inter = generate_text_interpolation(model1, (\"the\", \"quick\", \"brown\"), best_lambdas, best_k, 20)\n",
    "print('  '.join(gen_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_backoff(model, start_context, max_length=100):\n",
    "    contexts = list(start_context)\n",
    "    generated = contexts[:]\n",
    "    \n",
    "    for _ in range(max_length -  len(contexts)):\n",
    "        candidates = []\n",
    "        for word in model.ngram_counts[1]:\n",
    "            target_context = tuple(contexts + [word[0]])\n",
    "            prob = backoff_no_smooth_prob(model, target_context)\n",
    "            candidates.append((word, prob))\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_word = candidates[0][0]\n",
    "        if selected_word[0] == '<UNK>':\n",
    "            selected_word = candidates[1][0]\n",
    "\n",
    "        if selected_word[0] == '<END>':\n",
    "            break\n",
    "\n",
    "        if selected_word[0] == contexts[-1]:\n",
    "            selected_word = candidates[2][0]\n",
    "        \n",
    "        generated.append(selected_word[0])\n",
    "        contexts.append(selected_word[0])\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fox jumps over the of in . , . the , the , the , the , the ,\n"
     ]
    }
   ],
   "source": [
    "gt_backoff1 = generate_text_backoff(model1, test_ngrams, 20)\n",
    "print(\" \".join(gt_backoff1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown the touched the Mr. then throw is of receive In during it . directed the `` the\n"
     ]
    }
   ],
   "source": [
    "gt_backoff = generate_text_backoff(model1, (\"the\", \"quick\", \"brown\"), 20)\n",
    "print(\" \".join(gt_backoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a web UI that receive input text and give the next suggest word\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
