{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Text Generation on Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/maohieng/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 6127073\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK Brown Corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Combine all words into a single string\n",
    "corpus = ' '.join(brown.words())\n",
    "print('Corpus length:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 4288951\n",
      "Validation data length: 612707\n",
      "Test data length: 1225415\n",
      "[' ', ' ', 'i', 'i', '.', 'd', 's', '`', 'e', 'w']\n"
     ]
    }
   ],
   "source": [
    "# Split the corpus into training (70%), validation (10%), and test (20%) sets\n",
    "train_data, tmp_data = train_test_split(corpus, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(tmp_data, test_size=2/3, random_state=42)\n",
    "\n",
    "print('Train data length:', len(train_data))\n",
    "print('Validation data length:', len(val_data))\n",
    "print('Test data length:', len(test_data))\n",
    "print(train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens: ['g', 'u', 'r', 'S', '/', 'c', '}', '.', 'X', 'm']\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocabulary size to the 10000 most common words\n",
    "vocab_size = 10000\n",
    "train_counter = Counter(train_data)\n",
    "most_common_tokens = {word for word, _ in train_counter.most_common(vocab_size)}\n",
    "\n",
    "print('Most common tokens:', list(most_common_tokens)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the rare tokens with <UNK> token\n",
    "def replace_rare_tokens(data, most_common_tokens):\n",
    "    return [word if word in most_common_tokens else '<UNK>' for word in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training tokens: [' ', ' ', 'i', 'i', '.', 'd', 's', '`', 'e', 'w']\n",
      "Sample validation tokens: [' ', 'a', 't', 'c', 't', 'e', 'I', 'p', 'k', 'o']\n",
      "Sample test tokens: ['e', 'p', 't', 'e', '`', ' ', 'o', 'i', 'e', 't']\n"
     ]
    }
   ],
   "source": [
    "train_tokens_limited = replace_rare_tokens(train_data, most_common_tokens)\n",
    "val_tokens_limited = replace_rare_tokens(val_data, most_common_tokens)\n",
    "test_tokens_limited = replace_rare_tokens(test_data, most_common_tokens)\n",
    "\n",
    "print(\"Sample training tokens:\", train_tokens_limited[:10])\n",
    "print(\"Sample validation tokens:\", val_tokens_limited[:10])\n",
    "print(\"Sample test tokens:\", test_tokens_limited[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate n-grams\n",
    "def generate_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1-gram:\n",
      "[(1,), (2,), (3,), (4,), (5,)]\n",
      "Counter({(1,): 1, (2,): 1, (3,): 1, (4,): 1, (5,): 1})\n",
      "Sample 2-gram:\n",
      "[(1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "Counter({(1, 2): 1, (2, 3): 1, (3, 4): 1, (4, 5): 1})\n",
      "Sample 3-gram:\n",
      "[(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
      "Counter({(1, 2, 3): 1, (2, 3, 4): 1, (3, 4, 5): 1})\n"
     ]
    }
   ],
   "source": [
    "# Test generate_ngrams function\n",
    "test_tokens = [1, 2, 3, 4, 5]\n",
    "for i in range (1, 4):\n",
    "    print(f\"Sample {i}-gram:\")\n",
    "    ngrams = generate_ngrams(test_tokens, i)\n",
    "    print(ngrams)\n",
    "    counter_ngrams = Counter(ngrams)\n",
    "    print(counter_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Language Model 1: Backoff Method without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count n-grams\n",
    "n = 4\n",
    "ngram_counts = {}\n",
    "# context_counts = {}\n",
    "for i in range(1, n+1):\n",
    "    ngram_counts[i] = Counter(generate_ngrams(train_tokens_limited, i))\n",
    "    # context_counts[i] = Counter(generate_ngrams(train_tokens_limited, i-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of a word given the previous n-1 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Count the n-grams and their contexts in the training set\n",
    "ngram_counts = Counter(train_ngrams)\n",
    "context_counts = Counter([ngram[:-1] for ngram in train_ngrams])\n",
    "\n",
    "print(\"Sample n-gram counts:\")\n",
    "for ngram, count in list(ngram_counts.items())[:10]:\n",
    "    print(ngram, count)\n",
    "print(\"Sample context counts:\")\n",
    "for context, count in list(context_counts.items())[:10]:\n",
    "    print(context, count)\n",
    "\n",
    "# Estimate the probabilities of n-grams\n",
    "def estimate_probabilities(ngram_counts, context_counts):\n",
    "    ngram_probabilities = {}\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        context = ngram[:-1]\n",
    "        ngram_probabilities[ngram] = count / context_counts[context]\n",
    "    return ngram_probabilities\n",
    "\n",
    "ngram_probabilities = estimate_probabilities(ngram_counts, context_counts)\n",
    "\n",
    "print(\"Sample n-gram probabilities:\")\n",
    "for ngram, prob in list(ngram_probabilities.items())[:10]:\n",
    "    print(ngram, prob)\n",
    "\n",
    "# Compute the perplexity of the validation and test sets\n",
    "def compute_perplexity(ngrams, ngram_probabilities):\n",
    "    perplexity = 1\n",
    "    for ngram in ngrams:\n",
    "        perplexity *= 1 / ngram_probabilities.get(ngram, 1)\n",
    "    return pow(perplexity, 1/len(ngrams))\n",
    "\n",
    "val_perplexity = compute_perplexity(val_ngrams, ngram_probabilities)\n",
    "test_perplexity = compute_perplexity(test_ngrams, ngram_probabilities)\n",
    "\n",
    "print(\"Validation set perplexity:\", val_perplexity)\n",
    "print(\"Test set perplexity:\", test_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
