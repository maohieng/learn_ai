{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8 - Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "corpus = open('ex_8_3_corpus.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maohieng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text and create a single list of tokens\n",
    "tokens = []\n",
    "for text in corpus:\n",
    "    tokens.extend(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Get the top 50 most frequent words\n",
    "word_counts = Counter(tokens)\n",
    "top_words = [word for word, _ in word_counts.most_common(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'a', 't', 'n', 'i', 'o', 'r', 's', 'h', 'd', 'c', 'm', 'l', 'u', 'p', 'g', 'f', 'b', 'w', 'k', ',', 'y', '.', 'v', '1', '[', ']', '9', '5', '0', '3', '2', '4', '8', '(', '7', ')', '6', 'j', '``', '-', \"'\", 'q', 'é', '្', 'ា', 'â', 'ក', 'រ', 'x']\n"
     ]
    }
   ],
   "source": [
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Build the term-term co-occurrence matrix\n",
    "context_window = 4\n",
    "co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for i, word in enumerate(tokens):\n",
    "    if word in top_words:\n",
    "        for j in range(max(0, i - context_window), min(len(tokens), i + context_window + 1)):\n",
    "            if i != j and tokens[j] in top_words:\n",
    "                co_occurrence[word][tokens[j]] += 1\n",
    "\n",
    "# Convert the co-occurrence matrix into a DataFrame\n",
    "co_occurrence_matrix = pd.DataFrame.from_dict(co_occurrence, orient='index', columns=top_words).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           insert  content  wikipedia  article  first  second  third\n",
      "content       5.0      0.0        5.0      5.0    2.0     2.0    1.0\n",
      "first         2.0      2.0        1.0      1.0    0.0     0.0    0.0\n",
      "wikipedia     5.0      5.0        0.0      5.0    1.0     2.0    2.0\n",
      "article       5.0      5.0        5.0      0.0    1.0     2.0    2.0\n",
      "second        2.0      2.0        2.0      2.0    0.0     0.0    0.0\n",
      "third         1.0      1.0        2.0      2.0    0.0     0.0    0.0\n",
      "insert        0.0      5.0        5.0      5.0    2.0     2.0    1.0\n"
     ]
    }
   ],
   "source": [
    "print(co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the PPMI matrix\n",
    "def compute_ppmi(matrix):\n",
    "    total_sum = matrix.values.sum()\n",
    "    word_sums = matrix.sum(axis=1).values\n",
    "    context_sums = matrix.sum(axis=0).values\n",
    "    ppmi_matrix = matrix.copy()\n",
    "\n",
    "    for i, word_sum in enumerate(word_sums):\n",
    "        for j, context_sum in enumerate(context_sums):\n",
    "            joint_prob = matrix.iat[i, j] / total_sum\n",
    "            word_prob = word_sum / total_sum\n",
    "            context_prob = context_sum / total_sum\n",
    "\n",
    "            if joint_prob > 0:\n",
    "                ppmi = max(np.log2(joint_prob / (word_prob * context_prob)), 0)\n",
    "                ppmi_matrix.iat[i, j] = ppmi\n",
    "            else:\n",
    "                ppmi_matrix.iat[i, j] = 0\n",
    "\n",
    "    return ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_matrix = compute_ppmi(co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI Matrix:\n",
      "             insert   content  wikipedia   article     first    second  \\\n",
      "content    0.321928  0.000000   0.321928  0.321928  0.736966  0.321928   \n",
      "first      0.736966  0.736966   0.000000  0.000000  0.000000  0.000000   \n",
      "wikipedia  0.321928  0.321928   0.000000  0.321928  0.000000  0.321928   \n",
      "article    0.321928  0.321928   0.321928  0.000000  0.000000  0.321928   \n",
      "second     0.321928  0.321928   0.321928  0.321928  0.000000  0.000000   \n",
      "third      0.000000  0.000000   0.736966  0.736966  0.000000  0.000000   \n",
      "insert     0.000000  0.321928   0.321928  0.321928  0.736966  0.321928   \n",
      "\n",
      "              third  \n",
      "content    0.000000  \n",
      "first      0.000000  \n",
      "wikipedia  0.736966  \n",
      "article    0.736966  \n",
      "second     0.000000  \n",
      "third      0.000000  \n",
      "insert     0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Display the PPMI matrix\n",
    "print(\"PPMI Matrix:\")\n",
    "print(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence and PPMI matrices saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Save the matrices as CSV files\n",
    "co_occurrence_matrix.to_csv(\"co_occurrence_matrix.csv\", index=True)\n",
    "ppmi_matrix.to_csv(\"ppmi_matrix.csv\", index=True)\n",
    "\n",
    "print(\"Co-occurrence and PPMI matrices saved as CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Skip-gram with Negative Sampling (SGNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model using SGNS\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
